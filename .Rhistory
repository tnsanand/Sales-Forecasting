fit$centers
#See exactly which item are in 1st group
seed[fit$cluster==1,]
#See exactly which item are in 1st group
spikes[fit$cluster==1,]
#See exactly which item are in 1st group
head(spikes[fit$cluster==1,])
#See exactly which item are in 1st group
head(spikes[fit$cluster==1,])$Difference
#See exactly which item are in 1st group
max(spikes[fit$cluster==1,])$Difference
#See exactly which item are in 1st group
max((spikes[fit$cluster==1,])$Difference)
#See exactly which item are in 1st group
min((spikes[fit$cluster==1,])$Difference)
#See exactly which item are in 1st group
min((spikes[fit$cluster==2,])$Difference)
#See exactly which item are in 1st group
max((spikes[fit$cluster==2,])$Difference)
for (i in 1:21)
{
max((spikes[fit$cluster==i,])$Difference)
min((spikes[fit$cluster==2,])$Difference)
}
for (i in 1:21)
{
max((spikes[fit$cluster==i,])$Difference)
min((spikes[fit$cluster==2,])$Difference)
}
for (i in 1:21)
{
max((spikes[fit$cluster==i,])$Difference)
min((spikes[fit$cluster==i,])$Difference)
}
for (i in 1:21)
{
print(max((spikes[fit$cluster==i,])$Difference))
print(min((spikes[fit$cluster==i,])$Difference))
}
for (i in 1:21)
{
print("---------------------")
print("Program " + i)
print(max((spikes[fit$cluster==i,])$Difference))
print(min((spikes[fit$cluster==i,])$Difference))
print("---------------------")
}
for (i in 1:21)
{
print("---------------------")
print(i)
print(max((spikes[fit$cluster==i,])$Difference))
print(min((spikes[fit$cluster==i,])$Difference))
print("---------------------")
}
for (i in 1:21)
{
print("---------------------")
print(i)
print(max((spikes[fit$cluster==i,])$Difference))
print(min((spikes[fit$cluster==i,])$Difference))
print(avg((spikes[fit$cluster==i,])$Difference))
print("---------------------")
}
for (i in 1:21)
{
print("---------------------")
print(i)
print(max((spikes[fit$cluster==i,])$Difference))
print(min((spikes[fit$cluster==i,])$Difference))
print(mean((spikes[fit$cluster==i,])$Difference))
print("---------------------")
}
### 1. Read data into R.
spikes <- read.csv("C:/Users/Anand/Downloads/Python/SineWave/testdata.csv", header = TRUE)
spikes = spikes[order(spikes$SECOND_START),]
head(spikes[5])
spikes$Difference = cbind(spikes[5], rbind(0,apply(spikes[5],2,diff)))
max(spikes$Difference)
fit <- kmeans(spikes$Difference, 21)
fit$cluster
spikes$cluster = fit$cluster
plotcluster(spikes$Difference, spikes$cluster)
spikes$numericPrograms = as.numeric(spikes$Programs)
length(spikes$numericPrograms)
head(spikes[6])
plot(spikes$numericPrograms,spikes[6])
for (i in 1:21)
{
print("---------------------")
print(i)
print(max((spikes[fit$cluster==i,])$Difference))
print(min((spikes[fit$cluster==i,])$Difference))
print("---------------------")
}
spikes$numericPrograms
for (i in 1:21)
{
print("---------------------")
print(i)
print(max((spikes[fit$cluster==i,])$Difference))
print(min((spikes[fit$cluster==i,])$Difference))
print("---------------------")
}
for (i in 1:21)
{
print("---------------------")
print(i)
print(max((spikes[spikes$numericPrograms==i,])$Difference))
print(min((spikes[spikes$numericPrograms==i,])$Difference))
print("---------------------")
}
### 1. Read data into R.
spikes <- read.csv("C:/Users/Anand/Downloads/Python/SineWave/testdata.csv", header = TRUE)
spikes = spikes[order(spikes$SECOND_START),]
head(spikes[5])
spikes$Difference = cbind(spikes[5], rbind(0,apply(spikes[5],2,diff)))
max(spikes$Difference)
fit <- kmeans(spikes$Difference, 21)
fit$cluster
spikes$cluster = fit$cluster
plotcluster(spikes$Difference, spikes$cluster)
spikes$numericPrograms = as.numeric(spikes$Programs)
length(spikes$numericPrograms)
head(spikes[6])
plot(spikes$numericPrograms,spikes[6])
for (i in 1:21)
{
print("---------------------")
print(i)
print(max((spikes[fit$cluster==i,])$Difference))
print(min((spikes[fit$cluster==i,])$Difference))
print("---------------------")
}
for (i in 1:21)
{
print("---------------------")
print(i)
print(max((spikes[spikes$numericPrograms==i,])$Difference))
print(min((spikes[spikes$numericPrograms==i,])$Difference))
print("---------------------")
}
spikes$numericPrograms = as.numeric(spikes$Programs)
for (i in 1:21)
}
### 1. Read data into R.
spikes <- read.csv("C:/Users/Anand/Downloads/Python/SineWave/testdata.csv", header = TRUE)
spikes = spikes[order(spikes$SECOND_START),]
head(spikes[5])
spikes$Difference = cbind(spikes[5], rbind(0,apply(spikes[5],2,diff)))
max(spikes$Difference)
fit <- kmeans(spikes$Difference, 21)
fit$cluster
spikes$cluster = fit$cluster
plotcluster(spikes$Difference, spikes$cluster)
spikes$numericPrograms = as.numeric(spikes$Programs)
for (i in 1:21)
{
print("---------------------")
print(i)
print(max((spikes[fit$cluster==i,])$Difference))
print(min((spikes[fit$cluster==i,])$Difference))
print("---------------------")
}
for (i in 1:21)
{
print("---------------------")
print(i)
print(max((spikes[spikes$numericPrograms==i,])$Difference))
print(min((spikes[spikes$numericPrograms==i,])$Difference))
print("---------------------")
}
View(spikes)
spikes$numericPrograms
spikes$cluster
spikes$numericPrograms - spikes$cluster
---
title: "Sales Problem | BlackLocus"
Sales
setwd("C:/Users/Anand/Downloads/Sales")
Sales = read.table("SALES_QUESTION.csv", header=TRUE,sep=",")
head(Sales)
tail(Sales)
dim(Sales)
# Plot of the Data
plot(Sales)
abline(reg=lm(Sales$Sales~time(Sales$Date)))
```
Sales$Date = substr(Sales$Date,1,10)
Sales$Date = as.Date(as.character(Sales$Date),format="%Y-%m-%d")
require(xts)
Sales <- xts(Sales$Sales,Sales$Date)
require(forecast)
arima_fit <- auto.arima(Sales)
# Summary of the Time-Series Model fitted above
summary(arima_fit)
arima_fit <- auto.arima(Sales)
install.packages("forecast")
setwd("C:/Users/Anand/Downloads/Sales")
Sales = read.table("SALES_QUESTION.csv", header=TRUE,sep=",")
plot(Sales)
abline(reg=lm(Sales$Sales~time(Sales$Date)))
Sales$Date = substr(Sales$Date,1,10)
Sales$Date = as.Date(as.character(Sales$Date),format="%Y-%m-%d")
require(xts)
Sales <- xts(Sales$Sales,Sales$Date)
require(forecast)
arima_fit <- auto.arima(Sales)
# Summary of the Time-Series Model fitted above
summary(arima_fit)
arima_forecast <- forecast(arima_fit, h = 365)
# Forecast of values for the following year(2015) with 95% and 80% Confidence Intervals as well.
arima_forecast
plot(arima_forecast)
adf.test(diff(log(Sales$Sales)), alternative="stationary", k=0)
require(tseries)
adf.test(diff(log(Sales$Sales)), alternative="stationary", k=0)
adf.test(diff(log(Sales)), alternative="stationary", k=0)
acf(log(Sales))
acf(diff(log(Sales)))
acf(diff(log(Sales$Sales)))
acf(Sales)
acf(diff(Sales)
)
setwd("C:/Users/Anand/Downloads/Sales")
Sales = read.table("SALES_QUESTION.csv", header=TRUE,sep=",")
head(Sales); nrow(Sales)
plot(Sales)
hist(Sales$Sales)
# Now split the data 80/20 as training/testing datasets:
Sales$Date = substr(Sales$Date,1,10)
Sales$Date = as.Date(as.character(Sales$Date),format="%Y-%m-%d")
subset <- sample(nrow(Sales), nrow(Sales) * 0.8)
install.packages("forecast")
require(forecast)
require(xts)
Salesnew <- xts(Sales$Sales,Sales$Date)
arma_fit <- auto.arima(Salesnew)
summary(arma_fit)
arma_forecast <- forecast(arma_fit, h = 365)
arma_fit_accuracy <- accuracy(arma_forecast, test)
arma_fit; arma_forecast; arma_fit_accuracy
plot(arma_forecast)
lines(Salesnew)
install.packages("forecast")
plot(arma_forecast)
summary(arma_fit)
adf.test(diff(log(Sales)), alternative="stationary", k=0)
Sales$Date = as.Date(as.character(Sales$Date),format="%Y-%m-%d")
Sales$Date = as.Date(as.character(Sales$Date),format="%Y-%m-%d")
adf.test(diff(log(Sales)), alternative="stationary", k=0)
head(Sales)
tsdiag(arima_fit)
predict(arima_fit, Sales)
arima_fit$residuals
plot(Sales$Date,arima_fit$residuals)
plot(Sales$Date,arima_fit$residuals,ylab="Residuals", xlab="Sales Date")
qqplot(Sales,arima_forecast)
qqplot(Sales$Sales,arima_forecast$fitted)
qqplot(Sales$Sales,arima_fit$fitted)
qqnorm(arima_fit$residuals, asp = 1)
qqline(arima_fit$residuals, asp=1)
hist(arima_fit$residuals)
install.packages("forecast")
install.packages("forecast")
Sales$Sales
sales.s = diff(Sales$Sales)
summary(sales$sales)
summary(Sales$sales)
require(forecast)
adf.test(sales.s, k=0)
df
plot(sales.s)
plot(Sales$sales)
plot(Sales$Sales)
plot(Sales$Sales, cex = 0.7)
plot(Sales$Sales, cex = 0.1)
plot(Sales$Sales, cex = 0.2)
plot(Sales$Sales, cex = 0.2)
plot(Sales$Sales, cex = 0.4)
library(forecast)
install.packages("forecast")
install.packages("forecast")
adf.test(Sales, alternative="stationary", k=0)
require(tseries)
adf.test(Sales, alternative="stationary")
2-3
2-3
---
title: "Sales Problem | BlackLocus"
author: "Anand Kumar Taridalu Subrahmanyam"
date: "01st July, 2016"
output: html_document
---
**Summary of Observations**
The trend of Sales year by year is increasing approximately linearly, this states that the given Sales data is time dependent and is stationary.
For this Sales Data, I have using ARIMA modelling technique to forecast the following year's(2015) sales.
Auto-ARIMA model emphasizes on minimization of AICs and MLE to find the best model, by generating multiple models based on the differenncing factor obtained after stationarizing the data, and finds optimal values for p (AR) and q (MA) parameters.
ARIMA model, obtained in my case is ARIMA(0,1,2), which can be interpreted as follows, AR, p = 0, implies that this is not an AR(Auto-Regressive) model; d = 1, says that data needed one level of differencing to make it stationary; also q = 2, implies that it is MA(2), Moving Average model of second order.
Visualized the forecast of the model, for the next year and cross-validated that the model is working correctly and is in line with the trends observed initially.
Also, the Residuals versus Time plot implies that there is no significant pattern observed in residuals, which is desired.
This R Markdown document predicts the sales for the year, 2015 considering the historic data.
Set Working Directory and then Read the file which is located in the working Directory.
```{r echo=FALSE}
rm(list=ls())
```
```{r echo=FALSE}
setwd("C:/Users/Anand/Downloads/Sales")
Sales = read.table("SALES_QUESTION.csv", header=TRUE,sep=",")
```
**Plot and Explore Data**
Get a general idea on dataset's: Variables, dimensions, etc.,.
```{r}
head(Sales)
tail(Sales)
dim(Sales)
# Plot of the Data
plot(Sales)
abline(reg=lm(Sales$Sales~time(Sales$Date)))
```
The Sales data given is already sorted by date, and on observing the above plot of Sales data, we can interpret the following:
##### - The trend of Sales year by year is increasing gradually, this states that the given Sales data is time dependent and is stationary.
##### - Hence, the Sales Data has to be made stationary or time-independent before fitting any AR/MA models.
Here, we have **`r dim(Sales)[1]`** observations and **`r dim(Sales)[2]`** variables in the Sales dataset.
**Next,**
Histograms for Sales Data
```{r echo=FALSE}
differencedValue = diff(Sales$Sales)
hist(Sales$Sales)
```
On examining the distribution of Sales, it can be seen that the range of Sales is ~[10,50] and the distribution is satisfactorily normal in nature.
**Manually finding the differencing factor for this Data, as we would need the differecing level where the data becomes stationary, before fitting ARIMA models**
1) Plot Sales Data AS-IS
```{r}
plot(Sales$Sales, cex = 0.4)
```
From the above plot, we can observe a linear trend in the Sales data over time, which is not desired, as it implies time-dependency and non-stationarity, lets apply one level of differencing to Sales Data
2) Plot Sales Data with one level of differencing applied.
```{r}
plot(diff(Sales$Sales), cex = 0.4)
```
From, the above plot, we can infer that with one level of differencing the data, the linear trend observed initially vanished and the data has now turned stationary, for which ARIMA models can be applied.
The above differencing process is applied recursively until we obtain stationary data.
ADF Test is another option used to test the stationarity of data.
**Convert Data to Time Series Data**
```{r}
Sales$Date = substr(Sales$Date,1,10)
Sales$Date = as.Date(as.character(Sales$Date),format="%Y-%m-%d")
require(xts)
Sales <- xts(Sales$Sales,Sales$Date)
# Plot Time-Series Data, to observe potential trend
plot(Sales)
```
The above time-series plot is in line with the assumptions made w.r.t data earlier.
**ADF Test to Validate Stationarity**
```{r}
install.packages("forecast")
require(forecast)
require(tseries)
#  ADF Test to validate Stationarity of the Sales Data
adf.test(Sales, alternative="stationary", k = 1)
```
p-Value is within the thresholds hence with ADF-Test we can say reject the null hypothesis here, which says that the series is not-stationary. Hence, the series is stationary with one level of differencing.
**Model Fitting using Auto Arima, it Fits best ARIMA model to univariate time series**
Auto-ARIMA model emphasizes on minimization of AICs and MLE to find the best model
It selects the best model(chosses p and q) after finding qualifying value for d, differencing factor(I).
```{r}
arima_fit <- auto.arima(Sales)
# Summary of the Time-Series Model fitted above
summary(arima_fit)
```
For the above Auto-AR(I)MA model, we have the differencing factor of 1, which implies that the time series Sales Data, turned stationary after apply one level of differencing to the data, as validated above.
The optimal values of parameters, (p,d,q) for an ARIMA model is usually found after analyzing ACF and PACF plots. But, auto-ARIMA model takes care of optmizing these parametes by building multiple models and choosing the best model.
ARIMA model, obtained above is ARIMA(0,1,2), which can be interpreted as follows: AR, p = 0, implies that there is no AR(Auto-Regressive) component model; d = 1, says that data needed one level of differencing to make it stationary; also q = 2, implies that it is MA(2), Moving Average model of second order or MA model with 2 lags.
In my learning and understanding, I see that there is no effect of previous/historic valuesy(t-i) in predicting y(t) for this model, since p=0.
And, since q=2, this model is heavily dependent on the residuals from the previous two periods.
**Predictions for next 365 days**
```{r}
arima_forecast <- forecast(arima_fit, h = 365)
# Forecast of values for the following year(2015) with 95% and 80% Confidence Intervals as well.
arima_forecast
```
**Visualize the forecast of the model, for the following year to cross-validate that the model is working fine and is in line with the trends observed initially**
```{r}
plot(arima_forecast)
# Residual Plots
tsdiag(arima_fit)
plot(differencedValue,arima_fit$residuals,ylab="Residuals", xlab="Sales Date")
hist(arima_fit$residuals)
```
The analysis of above plots is given below.
1) The Time Series plot of the standardized residuals state that there is no trend observed in the residuals and approximately there is constant variance across time.
2) Also, the Residuals versus Time plot implies that there is no significant variance observed in residuals.
plot(differencedValue,arima_fit$residuals,ylab="Residuals", xlab="Sales Date")
differencedValue = diff(Sales$Sales)
---
title: "Sales Problem | BlackLocus"
author: "Anand Kumar Taridalu Subrahmanyam"
date: "01st July, 2016"
output: html_document
---
**Summary of Observations**
The trend of Sales year by year is increasing approximately linearly, this states that the given Sales data is time dependent and is stationary.
For this Sales Data, I have using ARIMA modelling technique to forecast the following year's(2015) sales.
Auto-ARIMA model emphasizes on minimization of AICs and MLE to find the best model, by generating multiple models based on the differenncing factor obtained after stationarizing the data, and finds optimal values for p (AR) and q (MA) parameters.
ARIMA model, obtained in my case is ARIMA(0,1,2), which can be interpreted as follows, AR, p = 0, implies that this is not an AR(Auto-Regressive) model; d = 1, says that data needed one level of differencing to make it stationary; also q = 2, implies that it is MA(2), Moving Average model of second order.
Visualized the forecast of the model, for the next year and cross-validated that the model is working correctly and is in line with the trends observed initially.
Also, the Residuals versus Time plot implies that there is no significant pattern observed in residuals, which is desired.
This R Markdown document predicts the sales for the year, 2015 considering the historic data.
Set Working Directory and then Read the file which is located in the working Directory.
```{r echo=FALSE}
rm(list=ls())
```
```{r echo=FALSE}
setwd("C:/Users/Anand/Downloads/Sales")
Sales = read.table("SALES_QUESTION.csv", header=TRUE,sep=",")
```
**Plot and Explore Data**
Get a general idea on dataset's: Variables, dimensions, etc.,.
```{r}
head(Sales)
tail(Sales)
dim(Sales)
# Plot of the Data
plot(Sales)
abline(reg=lm(Sales$Sales~time(Sales$Date)))
```
The Sales data given is already sorted by date, and on observing the above plot of Sales data, we can interpret the following:
##### - The trend of Sales year by year is increasing gradually, this states that the given Sales data is time dependent and is stationary.
##### - Hence, the Sales Data has to be made stationary or time-independent before fitting any AR/MA models.
Here, we have **`r dim(Sales)[1]`** observations and **`r dim(Sales)[2]`** variables in the Sales dataset.
**Next,**
Histograms for Sales Data
```{r echo=FALSE}
differencedValue = diff(Sales$Sales)
hist(Sales$Sales)
```
On examining the distribution of Sales, it can be seen that the range of Sales is ~[10,50] and the distribution is satisfactorily normal in nature.
**Manually finding the differencing factor for this Data, as we would need the differecing level where the data becomes stationary, before fitting ARIMA models**
1) Plot Sales Data AS-IS
```{r}
plot(Sales$Sales, cex = 0.4)
```
From the above plot, we can observe a linear trend in the Sales data over time, which is not desired, as it implies time-dependency and non-stationarity, lets apply one level of differencing to Sales Data
2) Plot Sales Data with one level of differencing applied.
```{r}
plot(diff(Sales$Sales), cex = 0.4)
```
From, the above plot, we can infer that with one level of differencing the data, the linear trend observed initially vanished and the data has now turned stationary, for which ARIMA models can be applied.
The above differencing process is applied recursively until we obtain stationary data.
ADF Test is another option used to test the stationarity of data.
**Convert Data to Time Series Data**
```{r}
Sales$Date = substr(Sales$Date,1,10)
Sales$Date = as.Date(as.character(Sales$Date),format="%Y-%m-%d")
require(xts)
Sales <- xts(Sales$Sales,Sales$Date)
# Plot Time-Series Data, to observe potential trend
plot(Sales)
```
The above time-series plot is in line with the assumptions made w.r.t data earlier.
**ADF Test to Validate Stationarity**
```{r}
install.packages("forecast")
require(forecast)
require(tseries)
#  ADF Test to validate Stationarity of the Sales Data
adf.test(Sales, alternative="stationary", k = 1)
```
p-Value is within the thresholds hence with ADF-Test we can say reject the null hypothesis here, which says that the series is not-stationary. Hence, the series is stationary with one level of differencing.
**Model Fitting using Auto Arima, it Fits best ARIMA model to univariate time series**
Auto-ARIMA model emphasizes on minimization of AICs and MLE to find the best model
It selects the best model(chosses p and q) after finding qualifying value for d, differencing factor(I).
```{r}
arima_fit <- auto.arima(Sales)
# Summary of the Time-Series Model fitted above
summary(arima_fit)
```
For the above Auto-AR(I)MA model, we have the differencing factor of 1, which implies that the time series Sales Data, turned stationary after apply one level of differencing to the data, as validated above.
The optimal values of parameters, (p,d,q) for an ARIMA model is usually found after analyzing ACF and PACF plots. But, auto-ARIMA model takes care of optmizing these parametes by building multiple models and choosing the best model.
ARIMA model, obtained above is ARIMA(0,1,2), which can be interpreted as follows: AR, p = 0, implies that there is no AR(Auto-Regressive) component model; d = 1, says that data needed one level of differencing to make it stationary; also q = 2, implies that it is MA(2), Moving Average model of second order or MA model with 2 lags.
In my learning and understanding, I see that there is no effect of previous/historic valuesy(t-i) in predicting y(t) for this model, since p=0.
And, since q=2, this model is heavily dependent on the residuals from the previous two periods.
**Predictions for next 365 days**
```{r}
arima_forecast <- forecast(arima_fit, h = 365)
# Forecast of values for the following year(2015) with 95% and 80% Confidence Intervals as well.
arima_forecast
```
**Visualize the forecast of the model, for the following year to cross-validate that the model is working fine and is in line with the trends observed initially**
```{r}
plot(arima_forecast)
# Residual Plots
tsdiag(arima_fit)
plot(differencedValue,arima_fit$residuals,ylab="Residuals", xlab="Sales Date")
hist(arima_fit$residuals)
```
The analysis of above plots is given below.
1) The Time Series plot of the standardized residuals state that there is no trend observed in the residuals and approximately there is constant variance across time.
2) Also, the Residuals versus Time plot implies that there is no significant variance observed in residuals.
install.packages("forecast")
differencedValue
plot(differencedValue,arima_fit$residuals,ylab="Residuals", xlab="Sales Date")
plot(xts(differencedValue),arima_fit$residuals,ylab="Residuals", xlab="Sales Date")
install.packages("ares")
install.packages("TSA")
library(TSA)
LB.test(arima_fit)
kpss.test(Sales)
adf.test(Sales)
kpss.test(Sales, null = "Trend")
arima_fit$residuals
