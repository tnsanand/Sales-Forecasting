---
title: "Sales Problem | BlackLocus"
author: "Anand Kumar Taridalu Subrahmanyam"
date: "01st July, 2016"
output: word_document
---
**Summary of Observations**

- The trend of Sales year by year is increasing linearly, this states that the given Sales data is time dependent and is not stationary in nature.

- For this Sales Data, I have using ARIMA modelling technique to forecast the following year's(2015) sales.

- ARIMA model, obtained in this case is ARIMA(0,1,2), which can be interpreted as follows, AR, p = 0, implies that there is no AR(Auto-Regressive) component in the model; d = 1, says that data needed one level of differencing to make it stationary; also q = 2, implies that it is MA(2), Moving Average model of second order.

- Auto-ARIMA model emphasizes on minimization of AICs and MLE to find the best model. It generates multiple models based on the differencing factor obtained after stationarizing the data, and finds optimal values for p (AR) and q (MA) parameters.

- Since, there is no auto-regressive component in the model, we can say that response variable, y(t) has no siginificant relationship with its previous/historic values. Also, the model is highly dependent on the previous two periods of the residuals.

- Visualized the forecast of the model, for the next year and cross-validated that the model is working correctly and is in line with the trends observed initially.

- At the end to validate the correctness of the model, I have applied Whitenoise test on residuals. White-noise test was applied using Ljung-Box test. And, it was observed that in this fitted model, the residuals are pretty close to white-noise.

This R Markdown document predicts the sales for the year, 2015 considering the historic data.

Set Working Directory and then Read the file which is located in the working Directory.

```{r echo=FALSE}
rm(list=ls())
```

```{r echo=FALSE}
setwd("C:/Users/Anand/Downloads/Sales")
Sales = read.table("SALES_QUESTION.csv", header=TRUE,sep=",")
```

**Plot and Explore Data**

Get a general idea on dataset's: Variables, dimensions, etc.,.

```{r}
head(Sales)
tail(Sales)
dim(Sales)

# Plot of the Data

plot(Sales)
abline(reg=lm(Sales$Sales~time(Sales$Date)))

```

The Sales data given is already sorted by date, and on observing the above plot of Sales data, we can interpret the following:

##### - The trend of Sales year by year is increasing gradually, this states that the given Sales data is time dependent and is non-stationary.

##### - Hence, the Sales Data has to be made stationary before fitting any AR/MA models.

##### - The Sales data spans for four years from 2011 through 2014.

Here, we have **`r dim(Sales)[1]`** observations and **`r dim(Sales)[2]`** variables in the Sales dataset.

**Next,**

Histograms for Sales Data

```{r echo=FALSE}

hist(Sales$Sales)

```

On examining the distribution of Sales, it can be seen that the range of Sales is ~[10,50] and the distribution though  normal, it is skewed a bit.

**Manually finding the differencing factor for this Data, as we would need the differecing level where the data becomes stationary, before fitting ARIMA models**

1) Plot Sales Data AS-IS

```{r}

plot(Sales$Sales, cex = 0.4)

```

From the above plot, we can observe a linear trend in the Sales data over time, which is not desired, as it implies time-dependency and non-stationarity, lets apply one level of differencing to Sales Data

2) Plot Sales Data with one level of differencing applied.

```{r}

plot(diff(Sales$Sales), cex = 0.4)

```

From, the above plot, we can infer that with one level of differencing the data, the linear trend observed initially vanished and the data has now turned stationary, for which ARIMA models can be applied.

The above differencing process is applied recursively until we obtain stationary data.

ADF Test is another option used to test the stationarity of data.

**Convert Data to Time Series Data**

```{r}
Sales$Date = substr(Sales$Date,1,10)
Sales$Date = as.Date(as.character(Sales$Date),format="%Y-%m-%d")

require(xts)

Sales <- xts(Sales$Sales,Sales$Date)

# Plot Time-Series Data, to observe potential trend

plot(Sales)

```

The above time-series plot is in line with the assumptions made w.r.t data earlier.

```{r}
install.packages("forecast")
require(forecast)
require(tseries)
```

**Model Fitting using Auto Arima, it fits the best ARIMA model to univariate time series data**

Auto-ARIMA model emphasizes on minimization of AICs and MLE to find the best model

It selects the best model(chosses p and q) after finding qualifying value for d, differencing factor(I).

```{r}
arima_fit <- auto.arima(Sales)

# Summary of the Time-Series Model fitted above

summary(arima_fit)

```

For the above Auto-AR(I)MA model, we have the differencing factor of 1, which implies that the time series Sales Data, turned stationary after apply one level of differencing to the data, as validated above.

The optimal values of parameters, (p,q) for an ARIMA model is usually found after analyzing ACF and PACF plots. But, auto-ARIMA model takes care of optmizing these parametes by building multiple models and choosing the best model.

ARIMA model, obtained above is ARIMA(0,1,2), which can be interpreted as follows: AR, p = 0, implies that there is no AR(Auto-Regressive) component model; d = 1, says that data needed one level of differencing to make it stationary; also q = 2, implies that it is MA(2), Moving Average model of second order or MA model with 2 lags.

In my learning and understanding, I see that there is no effect of previous/historic values in predicting the response variable for this model, as there is no auto-regressive component.

And, since q=2, this model is heavily dependent on the residuals from the previous two periods.

**Predictions for next 365 days**

```{r}
arima_forecast <- forecast(arima_fit, h = 365)

# Forecast of values for the following year(2015) with 95% and 80% Confidence Intervals as well.

arima_forecast

```

**Visualize the forecast of the model, for the following year(2015) to cross-validate that the model is working fine and is in line with the trends observed initially**

```{r}

plot(arima_forecast)

# Residual Plots

tsdiag(arima_fit)

# Histogram of Residuals

hist(arima_fit$residuals)

```

The histograms of residuals appear to be ~normal, however the right tail is a bit longer.

**Model Fit / WhiteNoise Test for Residuals**

**Test Used - Portmanteau test/Ljung-Box test**

```{r}

install.packages("TSA")
library(TSA)

LB.test(arima_fit)

```

From the p-value observed from the above test, we can infer that it is within the threshold limit and hence we can conclude that there are no significant auto-correlations or relationships found between the residuals of the fitted ARIMA model. Therefore, our Null Hypothesis: "Residuals are not white-noise" here, is rejected.

Thus, in this fitted model the residuals are pretty close to white-noise.
